{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hierarchical Softmax.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS4RXOokUHF_"
      },
      "source": [
        "import codecs\n",
        "from gensim import corpora\n",
        "from collections import  defaultdict, deque\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUuoNUtxVXQP"
      },
      "source": [
        "raw_text = [\"During my second month of nursing school, our professor gave us a pop quiz\", \n",
        "#             \"I was a conscientious student and had breezed through the questions, until I read the last one:\", \n",
        "# \"What is the first name of the woman who cleans the school?\",  \"Surely this was some kind of joke.\",\n",
        "# \"I had seen the cleaning woman several times. She was tall, dark-haired and in her 50s, but how would I know her name?\"\n",
        "]\n",
        "\n",
        "#  \"\"\"During my second month of nursing school, our professor gave us a pop quiz.  \n",
        "# I was a conscientious student and had breezed through the questions, until I read the last one: \n",
        "# “What is the first name of the woman who cleans the school?”  Surely this was some kind of joke. \n",
        "# I had seen the cleaning woman several times. She was tall, dark-haired and in her 50s, but how would I know her name?  \n",
        "# I handed in my paper, leaving the last question blank.  Before class ended, one student asked if the last question would count toward our quiz grade.  \n",
        "# “Absolutely,” said the professor.  “In your careers you will meet many people. All are significant. They deserve your attention and care, \n",
        "# even if all you do is smile and say ‘hello’. I’ve never forgotten that lesson. I also learned her name was Dorothy.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H688sejnxXv"
      },
      "source": [
        "class HuffmanNode:\n",
        "    def __init__(self, word_id, frequency):\n",
        "        self.word_id = word_id  \n",
        "        self.frequency = frequency  \n",
        "        self.left_child = None\n",
        "        self.right_child = None\n",
        "        self.father = None\n",
        "        self.Huffman_code = []  \n",
        "        self.path = []  \n",
        "\n",
        "\n",
        "class HuffmanTree:\n",
        "    def __init__(self, wordid_frequency_dict):\n",
        "        self.word_count = len(wordid_frequency_dict)  \n",
        "        self.wordid_code = {}\n",
        "        self.wordid_path = {}\n",
        "        self.root = None\n",
        "        unmerge_node_list = [HuffmanNode(wordid, frequency) for wordid, frequency in\n",
        "                             wordid_frequency_dict.items()]  \n",
        "        self.huffman = [HuffmanNode(wordid, frequency) for wordid, frequency in\n",
        "                        wordid_frequency_dict.items()]  \n",
        "        #huffman tree\n",
        "        self.build_tree(unmerge_node_list)\n",
        "        #huffman code\n",
        "        self.generate_huffman_code_and_path()\n",
        "\n",
        "    def merge_node(self, node1, node2):\n",
        "        sum_frequency = node1.frequency + node2.frequency\n",
        "        mid_node_id = len(self.huffman)  \n",
        "        father_node = HuffmanNode(mid_node_id, sum_frequency)\n",
        "        if node1.frequency >= node2.frequency:\n",
        "            father_node.left_child = node1\n",
        "            father_node.right_child = node2\n",
        "        else:\n",
        "            father_node.left_child = node2\n",
        "            father_node.right_child = node1\n",
        "        self.huffman.append(father_node)\n",
        "        return father_node\n",
        "\n",
        "    def build_tree(self, node_list):\n",
        "        while len(node_list) > 1:\n",
        "            i1 = 0  \n",
        "            i2 = 1  \n",
        "            if node_list[i2].frequency < node_list[i1].frequency:\n",
        "                [i1, i2] = [i2, i1]\n",
        "            for i in range(2, len(node_list)):\n",
        "                if node_list[i].frequency < node_list[i2].frequency:\n",
        "                    i2 = i\n",
        "                    if node_list[i2].frequency < node_list[i1].frequency:\n",
        "                        [i1, i2] = [i2, i1]\n",
        "            father_node = self.merge_node(node_list[i1], node_list[i2])  \n",
        "            if i1 < i2:\n",
        "                node_list.pop(i2)\n",
        "                node_list.pop(i1)\n",
        "            elif i1 > i2:\n",
        "                node_list.pop(i1)\n",
        "                node_list.pop(i2)\n",
        "            else:\n",
        "                raise RuntimeError('i1 should not be equal to i2')\n",
        "            node_list.insert(0, father_node)  \n",
        "        self.root = node_list[0]\n",
        "\n",
        "    def generate_huffman_code_and_path(self):\n",
        "        stack = [self.root]\n",
        "        while len(stack) > 0:\n",
        "            node = stack.pop()\n",
        "            \n",
        "            while node.left_child or node.right_child:\n",
        "                code = node.Huffman_code\n",
        "                path = node.path\n",
        "                node.left_child.Huffman_code = code + [1]\n",
        "                node.right_child.Huffman_code = code + [0]\n",
        "                node.left_child.path = path + [node.word_id]\n",
        "                node.right_child.path = path + [node.word_id]\n",
        "                \n",
        "                stack.append(node.right_child)\n",
        "                node = node.left_child\n",
        "            word_id = node.word_id\n",
        "            word_code = node.Huffman_code\n",
        "            word_path = node.path\n",
        "            self.huffman[word_id].Huffman_code = word_code\n",
        "            self.huffman[word_id].path = word_path\n",
        "            \n",
        "            self.wordid_code[word_id] = word_code\n",
        "            self.wordid_path[word_id] = word_path\n",
        "\n",
        "    \n",
        "    def get_all_pos_and_neg_path(self):\n",
        "        positive = []  \n",
        "        negative = [] \n",
        "        for word_id in range(self.word_count):\n",
        "            pos_id = []  \n",
        "            neg_id = []  \n",
        "            for i, code in enumerate(self.huffman[word_id].Huffman_code):\n",
        "                if code == 1:\n",
        "                    pos_id.append(self.huffman[word_id].path[i])\n",
        "                else:\n",
        "                    neg_id.append(self.huffman[word_id].path[i])\n",
        "            positive.append(pos_id)\n",
        "            negative.append(neg_id)\n",
        "        return positive, negative"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfjyswGon3b9"
      },
      "source": [
        "def test_tree(word_frequency):\n",
        "    print(\"word_frequency: \",word_frequency)\n",
        "    tree = HuffmanTree(word_frequency)\n",
        "    print('Word ID: ',tree.wordid_code)\n",
        "    print('Word ID Path: ',tree.wordid_path)\n",
        "    for i in range(len(word_frequency)):\n",
        "        print('Word ID: ',tree.huffman[i].path)\n",
        "    print('+ and - : ',tree.get_all_pos_and_neg_path())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_-7ydRCzSbG"
      },
      "source": [
        "word_to_id = {w:i for i, w in enumerate(set(raw_text))}\n",
        "id_to_word = {i:w for w, i in word_to_id.items()}\n",
        "word_frequency = {i:raw_text.count(w) for w,i in word_to_id.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hzg4znvvtpQ"
      },
      "source": [
        "word_freq = {}\n",
        "word_count_sum = 0\n",
        "sentence_count = 0\n",
        "\n",
        "for line in raw_text:\n",
        "    line = line.strip().split(' ')\n",
        "    word_count_sum += len(line)\n",
        "    sentence_count += 1\n",
        "\n",
        "    for word in line:\n",
        "        try:\n",
        "            word_freq[word] += 1\n",
        "        except:\n",
        "            word_freq[word] = 1\n",
        "word_id = 0\n",
        "\n",
        "id2word_dict = {}\n",
        "word2id_dict = {}\n",
        "wordId_frequency_dict = {}\n",
        "\n",
        "min_count = 1\n",
        "for per_word, per_count in word_freq.items():\n",
        "    if per_count < min_count:  \n",
        "        word_count_sum -= per_count\n",
        "        continue\n",
        "    id2word_dict[word_id] = per_word\n",
        "    word2id_dict[per_word] = word_id\n",
        "    wordId_frequency_dict[word_id] = per_count\n",
        "    word_id += 1\n",
        "word_count = len(word2id_dict)\n",
        "\n",
        "# word_freq, word_count_sum, sentence_count\n",
        "# word_count, id2word_dict, word2id_dict, wordId_frequency_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGk0AeKT-YzN"
      },
      "source": [
        "class InputData:\n",
        "    def __init__(self, raw_text, min_count):\n",
        "       \n",
        "        self.input_file = raw_text\n",
        "        self.min_count = min_count  \n",
        "        self.wordId_frequency_dict = {}  \n",
        "        self.word_count = 0  \n",
        "        self.word_count_sum = 0  \n",
        "        self.sentence_count = 0  \n",
        "        self.id2word_dict = {}  \n",
        "        self.word2id_dict = {}  \n",
        "        self._init_dict()  \n",
        "        self.sample_table = []\n",
        "        self._init_sample_table()  \n",
        "\n",
        "        self.huffman_tree = HuffmanTree(self.wordId_frequency_dict)\n",
        "        self.huffman_pos_path, self.huffman_neg_path = self.huffman_tree.get_all_pos_and_neg_path()\n",
        "\n",
        "        self.word_pairs_queue = deque()\n",
        "      \n",
        "        print('Word Count is:', self.word_count)\n",
        "        print('Word Count Sum is', self.word_count_sum)\n",
        "        print('Sentence Count is:', self.sentence_count)\n",
        "        print('Word Frequency:', self.wordId_frequency_dict)\n",
        "\n",
        "    def _init_dict(self):\n",
        "        word_freq = {}\n",
        "        \n",
        "        for line in self.input_file:\n",
        "            line = line.strip().split(' ')  \n",
        "            self.word_count_sum += len(line)\n",
        "            self.sentence_count += 1\n",
        "            for word in line:\n",
        "                try:\n",
        "                    word_freq[word] += 1\n",
        "                except:\n",
        "                    word_freq[word] = 1\n",
        "        word_id = 0\n",
        "        \n",
        "        for per_word, per_count in word_freq.items():\n",
        "            if per_count < self.min_count:  \n",
        "                self.word_count_sum -= per_count\n",
        "                continue\n",
        "            self.id2word_dict[word_id] = per_word\n",
        "            self.word2id_dict[per_word] = word_id\n",
        "            self.wordId_frequency_dict[word_id] = per_count\n",
        "            word_id += 1\n",
        "        self.word_count = len(self.word2id_dict)\n",
        "\n",
        "    def _init_sample_table(self):\n",
        "        sample_table_size = 1e8\n",
        "        pow_frequency = np.array(list(self.wordId_frequency_dict.values())) ** 0.75  \n",
        "        word_pow_sum = sum(pow_frequency)  \n",
        "        ratio_array = pow_frequency / word_pow_sum  \n",
        "        word_count_list = np.round(ratio_array * sample_table_size)\n",
        "        for word_index, word_freq in enumerate(word_count_list):\n",
        "            self.sample_table += [word_index] * int(word_freq)  \n",
        "        self.sample_table = np.array(self.sample_table)\n",
        "\n",
        "    \n",
        "    def get_batch_pairs(self, batch_size, window_size):\n",
        "        while len(self.word_pairs_queue) < batch_size:\n",
        "            for _ in range(10000):  \n",
        "                \n",
        "                sentence = self.input_file\n",
        "                if sentence is None or sentence == '':\n",
        "                    continue\n",
        "                wordId_list = []  \n",
        "                for words in sentence:\n",
        "                    words = words.strip().split(' ')\n",
        "                    for word in words:\n",
        "                        try:\n",
        "                            word_id = self.word2id_dict[word]\n",
        "                            wordId_list.append(word_id)\n",
        "                        except:\n",
        "                            continue\n",
        "\n",
        "                for i, wordId_w in enumerate(wordId_list):\n",
        "                    for j, wordId_v in enumerate(wordId_list[max(i - window_size, 0):i + window_size + 1]):\n",
        "                        assert wordId_w < self.word_count\n",
        "                        assert wordId_v < self.word_count\n",
        "                        if i == j:  \n",
        "                            continue\n",
        "                        self.word_pairs_queue.append((wordId_w, wordId_v))\n",
        "        result_pairs = []  \n",
        "        for _ in range(batch_size):\n",
        "            result_pairs.append(self.word_pairs_queue.popleft())\n",
        "        return result_pairs\n",
        "\n",
        "    def get_pairs(self, pos_pairs):\n",
        "        neg_word_pair = []\n",
        "        pos_word_pair = []\n",
        "        for pair in pos_pairs:\n",
        "            pos_word_pair += zip([pair[0]] * len(self.huffman_pos_path[pair[1]]), self.huffman_pos_path[pair[1]])\n",
        "            neg_word_pair += zip([pair[0]] * len(self.huffman_neg_path[pair[1]]), self.huffman_neg_path[pair[1]])\n",
        "        return pos_word_pair, neg_word_pair\n",
        "\n",
        "    def get_negative_sampling(self, positive_pairs, neg_count):\n",
        "        neg_u = np.random.choice(self.sample_table, size=(len(positive_pairs), neg_count)).tolist()\n",
        "        return neg_u\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate_pairs_count(self, window_size):\n",
        "        return self.word_count_sum * (2 * window_size - 1) - (self.sentence_count - 1) * (1 + window_size) * window_size\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUD1kTZuCtHi"
      },
      "source": [
        "def test():\n",
        "    test_data = InputData(raw_text, 1)\n",
        "    test_data.evaluate_pairs_count(2)\n",
        "    pos_pairs = test_data.get_batch_pairs(10, 2)\n",
        "    \n",
        "    pos_word_pairs = []\n",
        "    for pair in pos_pairs:\n",
        "        pos_word_pairs.append((test_data.id2word_dict[pair[0]], test_data.id2word_dict[pair[1]]))\n",
        "\n",
        "    neg_pair = test_data.get_negative_sampling(pos_pairs, 2)\n",
        "\n",
        "    neg_word_pair = []\n",
        "    for pair in neg_pair:\n",
        "        neg_word_pair.append((test_data.id2word_dict[pair[0]], test_data.id2word_dict[pair[1]]))\n",
        "\n",
        "    print()\n",
        "    print(\"=====================================\")\n",
        "    print()\n",
        "    return test_data.get_pairs(pos_pairs), pos_pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfFR0iqDXJO4",
        "outputId": "2b228c88-cf3f-48a9-9cbb-3d4969d6568a"
      },
      "source": [
        "test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word Count is: 14\n",
            "Word Count Sum is 14\n",
            "Sentence Count is: 1\n",
            "Word Frequency: {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1}\n",
            "\n",
            "=====================================\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(([(0, 24),\n",
              "   (0, 15),\n",
              "   (1, 14),\n",
              "   (1, 24),\n",
              "   (1, 15),\n",
              "   (1, 24),\n",
              "   (2, 14),\n",
              "   (2, 24),\n",
              "   (2, 24),\n",
              "   (2, 23),\n",
              "   (2, 16)],\n",
              "  [(0, 26),\n",
              "   (0, 24),\n",
              "   (0, 14),\n",
              "   (0, 26),\n",
              "   (0, 23),\n",
              "   (1, 26),\n",
              "   (1, 24),\n",
              "   (1, 26),\n",
              "   (1, 23),\n",
              "   (1, 26),\n",
              "   (1, 23),\n",
              "   (1, 15),\n",
              "   (2, 26),\n",
              "   (2, 24),\n",
              "   (2, 26),\n",
              "   (2, 24),\n",
              "   (2, 14),\n",
              "   (2, 26),\n",
              "   (2, 23),\n",
              "   (2, 15),\n",
              "   (2, 26),\n",
              "   (3, 26),\n",
              "   (3, 24),\n",
              "   (3, 14)]),\n",
              " [(0, 1),\n",
              "  (0, 2),\n",
              "  (1, 0),\n",
              "  (1, 2),\n",
              "  (1, 3),\n",
              "  (2, 0),\n",
              "  (2, 1),\n",
              "  (2, 3),\n",
              "  (2, 4),\n",
              "  (3, 1)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROaZap13_owV"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self, emb_size, emb_dimension):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.w_embeddings = nn.Embedding(2*emb_size-1, emb_dimension, sparse=True)\n",
        "        self.v_embeddings = nn.Embedding(2*emb_size-1, emb_dimension, sparse=True)\n",
        "        self._init_emb()\n",
        "\n",
        "    def _init_emb(self):\n",
        "        initrange = 0.5 / self.emb_dimension\n",
        "        self.w_embeddings.weight.data.uniform_(-initrange, initrange)\n",
        "        self.v_embeddings.weight.data.uniform_(-0, 0)\n",
        "\n",
        "    def forward(self, pos_w, pos_v,neg_w, neg_v):\n",
        "        \n",
        "        emb_w = self.w_embeddings(torch.LongTensor(pos_w))  \n",
        "        neg_emb_w = self.w_embeddings(torch.LongTensor(neg_w))\n",
        "        emb_v = self.v_embeddings(torch.LongTensor(pos_v))\n",
        "        neg_emb_v = self.v_embeddings(torch.LongTensor(neg_v))  \n",
        "\n",
        "        score = torch.mul(emb_w, emb_v).squeeze()\n",
        "        score = torch.sum(score, dim=1)\n",
        "        score = F.logsigmoid(-1 * score)\n",
        "        neg_score = torch.mul(neg_emb_w, neg_emb_v).squeeze()\n",
        "        neg_score = torch.sum(neg_score, dim=1)\n",
        "        neg_score = F.logsigmoid(neg_score)\n",
        "        \n",
        "        loss = -1 * (torch.sum(score) + torch.sum(neg_score))\n",
        "        return loss\n",
        "\n",
        "    def save_embedding(self, id2word, file_name):\n",
        "        embedding = self.w_embeddings.weight.data.numpy()\n",
        "        fout = open(file_name, 'w')\n",
        "        fout.write('%d %d\\n' % (len(id2word), self.emb_dimension))\n",
        "        for wid, w in id2word.items():\n",
        "            e = embedding[wid]\n",
        "            e = ' '.join(map(lambda x: str(x), e))\n",
        "            fout.write('%s %s\\n' % (w, e))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRDMQjkJFz8X"
      },
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "WINDOW_SIZE = 4  \n",
        "BATCH_SIZE = 64  \n",
        "MIN_COUNT = 3  \n",
        "EMB_DIMENSION = 100  \n",
        "LR = 0.02  \n",
        "NEG_COUNT = 4 \n",
        "\n",
        "\n",
        "class Word2Vec:\n",
        "    def __init__(self, input_file_name, output_file_name):\n",
        "        self.output_file_name = output_file_name\n",
        "        self.data = InputData(input_file_name, MIN_COUNT)\n",
        "        self.model = SkipGramModel(self.data.word_count, EMB_DIMENSION)\n",
        "        self.lr = LR\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "    def train(self):\n",
        "        print(\"SkipGram Training......\")\n",
        "        pairs_count = self.data.evaluate_pairs_count(WINDOW_SIZE)\n",
        "        print(\"pairs_count\", pairs_count)\n",
        "        batch_count = pairs_count / BATCH_SIZE\n",
        "        print(\"batch_count\", batch_count)\n",
        "        process_bar = tqdm(range(int(batch_count)))\n",
        "        for i in process_bar:\n",
        "            pos_pairs = self.data.get_batch_pairs(BATCH_SIZE, WINDOW_SIZE)\n",
        "            pos_pairs,neg_pairs = self.data.get_pairs(pos_pairs)\n",
        "            pos_u = [pair[0] for pair in pos_pairs]\n",
        "            pos_v = [int(pair[1]) for pair in pos_pairs]\n",
        "            neg_u = [pair[0] for pair in neg_pairs]\n",
        "            neg_v = [int(pair[1]) for pair in neg_pairs]\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = self.model.forward(pos_u, pos_v, neg_u,neg_v)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if i * BATCH_SIZE % 100000 == 0:\n",
        "                self.lr = self.lr * (1.0 - 1.0 * i / batch_count)\n",
        "                for param_group in self.optimizer.param_groups:\n",
        "                    param_group['lr'] = self.lr\n",
        "\n",
        "        self.model.save_embedding(self.data.id2word_dict, self.output_file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtoxRj1mGUpO",
        "outputId": "cf714d86-0082-42a7-a086-c979ffa96693"
      },
      "source": [
        "w2v = Word2Vec(input_file_name=raw_text, output_file_name=\"word_embedding.txt\")\n",
        "w2v.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Word Count is: 4\n",
            "Word Count Sum is 16\n",
            "Sentence Count is: 5\n",
            "SkipGram Training......\n",
            "pairs_count 32\n",
            "batch_count 0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzRqIIcQwGc_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}